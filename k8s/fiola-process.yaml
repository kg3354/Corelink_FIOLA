# # apiVersion: batch/v1
# # kind: Job
# # metadata:
# #   name: fiola-process-job
# # spec:
# #   template:
# #     spec:
# #       restartPolicy: Never
# #       containers:
# #       - name: fiola-init-container
# #         image: registry.hsrn.nyu.edu/vip/corelink-examples/fiola-process:latest
# #         command: ["sh", "-c"]
# #         args:
# #           - |
# #             python ./receive_then_fiola.py
# #             tail -f /dev/null
# #         resources:
# #           limits:
# #             nvidia.com/gpu: 1 # Request 1 NVIDIA GPU
# #       tolerations:
# #       - key: nvidia.com/gpu
# #         operator: Exists
# #         effect: NoSchedule
# #   backoffLimit: 0


# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: fiola-process-job
# spec:
#   template:
#     spec:
#       restartPolicy: Never
#       containers:
#         - name: fiola-init-container
#           image: registry.hsrn.nyu.edu/vip/corelink-examples/fiola-process-ns:latest
#           command: ["sh", "-c"]
#           args:
#             - |
#               python ./receive_then_fiola.py
#               tail -f /dev/null
#           resources:
#             limits:
#               nvidia.com/gpu: 2 # Limit 1 NVIDIA GPU
#               cpu: "10000m"          
#               memory: "128Gi"
#             requests:
#               nvidia.com/gpu: 2 # Request 1 NVIDIA GPU
#               cpu: "10000m"    
#               memory: "128Gi"     
#       nodeSelector:
#         topology.kubernetes.io/zone: "meyer"
#       tolerations:
#         - key: nvidia.com/gpu
#           operator: Exists
#           effect: NoSchedule
#   backoffLimit: 0
apiVersion: batch/v1
kind: Job
metadata:
  name: fiola-process-job
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: fiola-init-container
          image: registry.hsrn.nyu.edu/vip/corelink-examples/fiola-process-ns:latest
          command: ["sh", "-c"]
          args:
            - |
              python ./receive_then_fiola.py
              tail -f /dev/null
          resources:
            limits:
              # nvidia.com/gpu: 1  # Limit 2 NVIDIA GPUs
              cpu: "20000m"      # Limit 10 CPUs
              memory: "128Gi"    # Limit 128 GiB memory
            requests:
              # nvidia.com/gpu: 1  # Request 2 NVIDIA GPUs
              cpu: "20000m"      # Request 10 CPUs
              memory: "128Gi"    # Request 128 GiB memory
      nodeSelector:
        topology.kubernetes.io/zone: "meyer"
      # tolerations:
      #   - key: nvidia.com/gpu
      #     operator: Exists
      #     effect: NoSchedule
  backoffLimit: 0
